import { Track } from "livekit-client"
import type { AudioProcessorOptions, Room, TrackProcessor } from "livekit-client"
import { DenoiseOptions } from "./options"
import { DeepFilterLoader } from "./deepfilternet/DeepFilterLoader"

const DenoiseWorkletCode = process.env.DENOISER_WORKLET

export type DenoiseFilterOptions = DenoiseOptions

export class DenoiseProcessor implements TrackProcessor<Track.Kind.Audio, AudioProcessorOptions> {
    static #hasloadWorkModule: boolean = false;

    readonly name = "denoise-filter";
    processedTrack?: MediaStreamTrack | undefined;

    private audioOpts?: AudioProcessorOptions;
    private filterOpts?: DenoiseFilterOptions;
    private isFilterEnabled: boolean = true;
    private audioSource?: MediaStreamAudioSourceNode | undefined;
    private denoiseNode?: AudioWorkletNode | undefined;

    constructor(options?: DenoiseFilterOptions) {
        this.filterOpts = options ?? {debugLogs: false, vadLogs: false, deepfilter: {enable: false}};
        console.log("DenoiseProcessor requested with options:", this.filterOpts);
    }

    async init(opts: AudioProcessorOptions) : Promise<void> {
        if (this.filterOpts?.debugLogs) {
            console.log("Initializing DenoiseProcessor with options:", opts);
        }
        await this._initInternal(opts, false);
    }

    async restart(opts: AudioProcessorOptions) : Promise<void> {
        // restart with empty audio context
        opts.audioContext = opts.audioContext ?? this.audioOpts?.audioContext;

        if (this.filterOpts?.debugLogs) {
            console.log("Restarting DenoiseProcessor with options:", opts);
        }
        await this._initInternal(opts, /*restart*/true);
    }

    async destroy() : Promise<void> {
        if (this.filterOpts?.debugLogs) {
            console.log("Destroying DenoiseProcessor");
        }

        this._closeInternal();
    }

    async onPublish(room: Room): Promise<void> {
       if (this.filterOpts?.debugLogs) {
            console.log("Publishing DenoiseProcessor to room:", room.name);
        }
    }

    async onUnpublish(): Promise<void> {
        if (this.filterOpts?.debugLogs) {
            console.log("Unpublishing DenoiseProcessor");
        }
    }

    static isSupported(): boolean {
        return true;
    }

    async setEnabled(enabled: boolean): Promise<void> {
        if (this.filterOpts?.debugLogs) {
            console.log("Setting DenoiseProcessor enabled state to:", enabled);
        }

        if (this.denoiseNode) {
            this.isFilterEnabled = enabled;
            this.denoiseNode.port.postMessage({ type: 'SET_ENABLED', enabled });
        }
    }

    async isEnabled(): Promise<boolean> {
        if (this.denoiseNode) {
            return this.isFilterEnabled;
        }
        return false;
    }

    async _initInternal(opts: AudioProcessorOptions, restart: boolean): Promise<void> {
        if (!opts || !opts.audioContext || !opts.track || !DenoiseWorkletCode) {
            throw new Error("audioContext and track are required");
        }

        if (restart) {
            this._closeInternal();
        }

        this.audioOpts = opts;

        if (DenoiseProcessor.#hasloadWorkModule === false) {
            const blob = new Blob([DenoiseWorkletCode], { type: 'application/javascript' });
            await this.audioOpts.audioContext?.audioWorklet.addModule((URL as any).createObjectURL(blob));

            DenoiseProcessor.#hasloadWorkModule = true;
        }

        // Load DeepFilter WASM data if DeepFilter is enabled
        let deepFilterWasmData;
        if (this.filterOpts?.deepfilter?.enable) {
            if (this.filterOpts?.debugLogs) {
                console.log("DenoiseProcessor: Loading DeepFilter WASM data...");
            }
            try {
                // Set custom base path if provided
                if (this.filterOpts?.wasmBasePath) {
                    if (this.filterOpts?.debugLogs) {
                        console.log("DenoiseProcessor: DeepFilterNet setting WASM base path to:", this.filterOpts.wasmBasePath);
                    }
                    DeepFilterLoader.setBasePath(this.filterOpts.wasmBasePath);
                }
                
                if (this.filterOpts?.debugLogs) {
                    console.log("DenoiseProcessor: Using WASM base path:", DeepFilterLoader.getBasePath());
                }
                
                await DeepFilterLoader.loadWasmData();
                deepFilterWasmData = DeepFilterLoader.getWasmData();
                if (this.filterOpts?.debugLogs) {
                    console.log("DenoiseProcessor: DeepFilter WASM data loaded successfully");        
                    console.log("DeepFilter: File sizes:", {
                        wasmJsCode: deepFilterWasmData?.wasmJsCode.length,
                        wasmBytes: deepFilterWasmData?.wasmBinary.byteLength,
                        modelBytes: deepFilterWasmData?.modelData.byteLength
                    });
                }
            } catch (error) {
                console.warn("DenoiseProcessor: Failed to load DeepFilter WASM data:", error, ". Falling back to RNNoise");
                
                // Don't throw here, let it fall back to RNNoise
                this.filterOpts.deepfilter = undefined; // Disable DeepFilter
            }
        }

        // Process node
        // Currently we only denoise and send back one channel even to stereo inputs&outputs. And different
        // browsers have different behaviors regarding the number of channels in the input&output:
        // - Chrome/Edge will capture mic in mono by default and callback with stereo inputs&outputs always.
        // - Firefox will capture mic in stereo by default and callback with input&output with same channels.
        // - Safari probably capture mic in mono by default and callback with stereo inputs&outputs always.
        // If the second channel is not populated, the browser perhaps will have one silent channel at rendering.
        // There are two ways to work around this if we are not to do denoising both channels:
        // 1. Request mono audio only via getUserMedia() if Rnnoise is enabled.
        // 2. Request mono output to AudioWorkletNode. Browser will do the mixing for us.
        try {
            this.denoiseNode = new AudioWorkletNode(this.audioOpts.audioContext, "DenoiseWorklet", {
                // By default, there is only one Input/Output.
                outputChannelCount: [1], // An array defining the number of channels for each output. 
                processorOptions: {
                    filterOpts: this.filterOpts,
                    deepFilterWasmData: deepFilterWasmData, // Pass pre-loaded WASM data
                }
            });
        } catch (error) {
            throw new Error(`Failed to create DenoiseWorklet: ${error}. Make sure the worklet module is properly loaded.`);
        }

        // Source node
        this.audioSource = this.audioOpts.audioContext.createMediaStreamSource(
            new MediaStream([this.audioOpts.track])
        );
        this.audioSource.connect(this.denoiseNode);

        // Destination node
        const destination = this.audioOpts.audioContext.createMediaStreamDestination();
        this.denoiseNode.connect(destination);

        this.processedTrack = destination.stream.getTracks()[0];

        if (this.filterOpts?.debugLogs) {
            console.log(`DenoiseProcessor init done: source track ID: ${this.audioOpts.track.id}, processed track ID: ${this.processedTrack.id}`);
        }
    }

    _closeInternal(): void {
        this.denoiseNode?.port.postMessage({ type: 'DESTROY' });
        this.denoiseNode?.disconnect();
        this.audioSource?.disconnect();
        this.processedTrack = undefined;
        this.denoiseNode = undefined;
        this.audioSource = undefined;
        this.audioOpts = undefined;
        
        DenoiseProcessor.#hasloadWorkModule = false;
    }
}